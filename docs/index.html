<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140332927-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-140332927-1');
</script>

<title>FUNIT Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="images/teaser_fb.jpg"/>
<meta property="og:title" content="Few-Shot Unsupervised Image-to-Image Translation"/>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script>	
</head>

<body>

<div id="primarycontent">
<center><h1>Few-Shot Unsupervised Image-to-Image Translation</h1></center>
<center><h2>
	<a href="http://mingyuliu.net/">Ming-Yu Liu</a>&nbsp;&nbsp;&nbsp;
	<a href="http://www.cs.cornell.edu/~xhuang/">Xun Huang</a>&nbsp;&nbsp;&nbsp;
	<a href="http://arunmallya.github.io/">Arun Mallya</a>&nbsp;&nbsp;&nbsp;
	<a href="https://research.nvidia.com/person/tero-karras">Tero Karras</a>&nbsp;&nbsp;&nbsp;
	<a href="https://users.aalto.fi/~ailat1/">Timo Aila</a>&nbsp;&nbsp;&nbsp;
	<a href="https://users.aalto.fi/~lehtinj7/">Jaakko Lehtinen</a>&nbsp;&nbsp;&nbsp;
	<a href="http://jankautz.com/">Jan Kautz</a>&nbsp;&nbsp;&nbsp;
	</h2>
	<center><h2>
		<a href="https://www.nvidia.com/en-us/">NVIDIA</a>&nbsp;&nbsp;&nbsp;
	</h2></center>
<center><h2>in ICCV 2019</h2></center>
<center><h2><strong><a href="https://arxiv.org/abs/1905.01723">Paper</a> | <a href="https://github.com/nvlabs/FUNIT/">Code</a> | <a href="petswap.html">Demo</a> </strong> </h2></center>
<center><a href="https://www.youtube.com/embed/FYQ7d1Ja-HY">
<img src="images/animal_8x8.gif" width="99%"> </a></center>
<p></p>
<br>
<h2 align="center">Abstract</h2>

<div style="font-size:14px"><p align="justify">Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework.</p></div>


<a href="https://arxiv.org/abs/1905.01723"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.jpg" width=170></a>



<h2>Paper</h2>
<p><a href="https://arxiv.org/abs/1905.01723">arxiv</a>,  2019. </p>



<h2>Citation</h2>
<p>Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz.<br>"Few-shot Unsupervised Image-to-Image Translation", in ICCV, 2019.
<a href="FUNIT.txt">Bibtex</a>

</p>


<h2>Code </h2> <p><a href='https://github.com/NVLabs/FUNIT'> PyTorch </a></p>


<br>

<table border="0" cellspacing="0" cellpadding="10" width="100%">
	<tr>
	<td align="center" valign="middle" width="50%" class="full">
		<h2>  FUNIT Explained </h2>
		<p><iframe width="100%" height="300px" src="https://www.youtube.com/embed/kgPAqsC8PLM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
	</td>
	
	<td align="center" valign="middle" width="50%" class="full">
		<h2> PetSwap Demo </h2>
		<p><iframe width="100%" height="300px" src="https://www.youtube.com/embed/JTu-U0C4xEU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
	</td>

	</tr>
</table>

<br>
<h1 align='center'> Problem Setting </h1>
<center><img src="images/problem.jpg" width="1000"></center>
<br>
	<p align="justify"> While unsupervised/unpaired image-to-image translation methods (e.g., <a href="https://arxiv.org/abs/1606.07536"><span style="font-weight:normal">Liu and Tuzel</span></a>, <a href="https://arxiv.org/abs/1703.00848"><span style="font-weight:normal">Liu et. al.</span></a>, <a href="https://arxiv.org/abs/1703.10593"><span style="font-weight:normal">Zhu et. al.</span></a>, and <a href="https://arxiv.org/abs/1804.04732"><span style="font-weight:normal">Huang et. al.</span></a>) have achieved remarkable success, they are still limited in two aspects. </p>
<div align="left">
	<li font-size: 15px> First, they generally require seeing a lot of images from target class in the training time.
</li>
	<li font-size: 15px> Second, a trained model for a translation task cannot be repurposed for another translation task in the test time.</li>
</div>
	<br>
	<p align="justify">
We propose a few-shot unsupervised image-to-image translation framework (FUNIT) to address the limitation. In the training time, the FUNIT model learns to translate images between any two classes sampled from a set of source classes. In the test time, the model is presented a few images of a target class that the model has never seen before. The model leverages these few example images to translate an input image of a source class to the target class.
	</p>


<br>
<h1 align='center'> Few-shot Unsupervised Image-to-Image Translation Examples</h1>
	<p align="justify">We show results on animal face translation, bird translation, flower translation, and food translation. For each example, </p>
<div align="left">
  <li><b>y1</b> and <b>y2</b> are the few-shot example images of the target class made available during testing,</li>
  <li><b>x</b> is the input image of the source class, and</li>
  <li><b>x bar</b> is the translation from the source class to the target class.</li>
<div/>
	<br>
	<p>
	Our model is able to translate a leopard to a sharpei even though it has never seen a single image of a sharpei in the training time.
	</p>
<center><img src="images/funit_example.jpg" width="1000"></center>
</body></html
>
